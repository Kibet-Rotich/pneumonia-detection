{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/R/9/joe/Rt3puCt7nGp6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kibet-Rotich/pneumonia-detection/blob/master/Pneumonia_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "doctors or other experts in hospitals are usually required to examine xray slides to diagnose patients like pneumonia patients. this process is usually error prone and requires doctors to use valuable time to diagnose even healthy exrays, and with the current shortage of doctors in kenya, everytime saved can be used by the doctor to serve patients better elsewhere.\n",
        "\n",
        "this solution is a pneumonia predictor, where you can feed in the image of a chest exray and its going to predict the chances of the patient having pneumonia.\n",
        " the solution should act as an auxilliary tool, ruling out the obvious healthy xrays and ensuring the doctor only uses their time todiagnose sick patients.\n",
        "\n",
        " it also acts as a proof of concept that computer vision can be used to detect any diseases that can be seen through an xray ctscan or mri scan."
      ],
      "metadata": {
        "id": "kfwIs0f_WhFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data, the data we have are xray images from normal scans and from pneumonia patients. the dataset has images categorized like this.\n",
        "test normal 234 pneumonia 390\n",
        "train normal 1341 pneumonia 3875\n",
        "val normal 8 pneumonia 8\n"
      ],
      "metadata": {
        "id": "GPYZ_9_YiajM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing, we are going to augment the images to ensure model doesnt overfit and the distribution is more equal, its also going to generate more photos which is good."
      ],
      "metadata": {
        "id": "PUmE2gJ7o7i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset folder\n",
        "data_dir = '/content/drive/MyDrive/datasets'\n",
        "\n",
        "# Check if it worked\n",
        "print(os.listdir(data_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A37gIdPViZ-2",
        "outputId": "e473e931-1d5c-4230-f81b-742979f823bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Pneumonia prediction', 'pneumonia_cnn_model.h5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQvOm4b3slO8",
        "outputId": "e81bb20c-2cc9-4185-ffe5-47b822f3513e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Base directory\n",
        "base_dir = \"/content/drive/MyDrive/datasets/Pneumonia prediction\"\n",
        "\n",
        "train_dir = base_dir + \"/train\"\n",
        "val_dir   = base_dir + \"/val\"\n",
        "test_dir  = base_dir + \"/test\"\n",
        "\n",
        "# Parameters\n",
        "img_size = (150, 150)  # You can try (224,224) if using transfer learning later\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Validation & test data should NOT be augmented\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow from directories\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85003f19",
        "outputId": "375f3361-3df5-4c8b-97ae-b416aff9ad24"
      },
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=10,\n",
        "    validation_data=val_gen\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1661s\u001b[0m 10s/step - accuracy: 0.7442 - loss: 0.5799 - val_accuracy: 0.7500 - val_loss: 0.6469\n",
            "Epoch 2/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 2s/step - accuracy: 0.8699 - loss: 0.3147 - val_accuracy: 0.9375 - val_loss: 0.4793\n",
            "Epoch 3/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 2s/step - accuracy: 0.8964 - loss: 0.2507 - val_accuracy: 0.8125 - val_loss: 0.6071\n",
            "Epoch 4/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 2s/step - accuracy: 0.9019 - loss: 0.2364 - val_accuracy: 0.7500 - val_loss: 0.6955\n",
            "Epoch 5/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 2s/step - accuracy: 0.9224 - loss: 0.1906 - val_accuracy: 0.6875 - val_loss: 1.1625\n",
            "Epoch 6/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 2s/step - accuracy: 0.9359 - loss: 0.1751 - val_accuracy: 0.9375 - val_loss: 0.2703\n",
            "Epoch 7/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 2s/step - accuracy: 0.9253 - loss: 0.1821 - val_accuracy: 0.8125 - val_loss: 0.4703\n",
            "Epoch 8/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 2s/step - accuracy: 0.9418 - loss: 0.1581 - val_accuracy: 0.7500 - val_loss: 0.6037\n",
            "Epoch 9/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 2s/step - accuracy: 0.9407 - loss: 0.1479 - val_accuracy: 0.8750 - val_loss: 0.3508\n",
            "Epoch 10/10\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 2s/step - accuracy: 0.9453 - loss: 0.1576 - val_accuracy: 0.8125 - val_loss: 0.4768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_gen)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmJyPkqhZ8L6",
        "outputId": "6c7c3008-e4a5-426b-b426-b0fca63cccf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 11s/step - accuracy: 0.6307 - loss: 0.9137\n",
            "Test Accuracy: 0.7964743375778198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save(\"/content/drive/MyDrive/datasets/pneumonia_cnn_model.h5\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ONjOpk8c9C4",
        "outputId": "8d8446e5-7889-473b-ea93-01546069348f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('pneumonia_resnet_model.keras')"
      ],
      "metadata": {
        "id": "0dBP7cd3dV8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Base parameters\n",
        "img_size = (224, 224)   # ResNet50 expects 224x224\n",
        "batch_size = 32\n",
        "\n",
        "# Use the same data generators but resize to 224x224\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Load pre-trained ResNet50 without the top layer\n",
        "base_model = ResNet50(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the base model layers (so they don’t retrain initially)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = layers.Flatten()(base_model.output)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train only the top layers\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=5  # try more (10–15) if GPU allows\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_gen)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFkHGeVLiLn5",
        "outputId": "4a456748-4f4e-43df-e7ad-a154eeace0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m969s\u001b[0m 6s/step - accuracy: 0.7156 - loss: 1.6146 - val_accuracy: 0.5000 - val_loss: 0.7079\n",
            "Epoch 2/5\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 732ms/step - accuracy: 0.7489 - loss: 0.5201 - val_accuracy: 0.5000 - val_loss: 0.7875\n",
            "Epoch 3/5\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 721ms/step - accuracy: 0.7510 - loss: 0.4904 - val_accuracy: 0.5000 - val_loss: 0.6412\n",
            "Epoch 4/5\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 720ms/step - accuracy: 0.7402 - loss: 0.4626 - val_accuracy: 0.5000 - val_loss: 0.7362\n",
            "Epoch 5/5\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 720ms/step - accuracy: 0.7391 - loss: 0.5192 - val_accuracy: 0.5000 - val_loss: 0.8505\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 7s/step - accuracy: 0.2984 - loss: 0.8606\n",
            "Test Accuracy: 0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze last few layers of ResNet\n",
        "for layer in base_model.layers[-30:]:  # last 30 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile again with a lower learning rate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history_finetune = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/datasets/pneumonia_resnet_finetuned 2.keras\")\n",
        "20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5fUofXso7lV",
        "outputId": "c1aa3470-ee5c-4adb-a9bb-3cc3997b2fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 760ms/step - accuracy: 0.9356 - loss: 0.1702 - val_accuracy: 0.6875 - val_loss: 1.1808\n",
            "Epoch 2/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 729ms/step - accuracy: 0.9386 - loss: 0.1732 - val_accuracy: 0.5625 - val_loss: 1.5546\n",
            "Epoch 3/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 725ms/step - accuracy: 0.9343 - loss: 0.1685 - val_accuracy: 0.8125 - val_loss: 0.3636\n",
            "Epoch 4/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 732ms/step - accuracy: 0.9360 - loss: 0.1525 - val_accuracy: 0.5625 - val_loss: 4.3266\n",
            "Epoch 5/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 735ms/step - accuracy: 0.9377 - loss: 0.1652 - val_accuracy: 0.8125 - val_loss: 0.6380\n",
            "Epoch 6/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 722ms/step - accuracy: 0.9423 - loss: 0.1412 - val_accuracy: 0.8125 - val_loss: 0.5120\n",
            "Epoch 7/20\n",
            "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 726ms/step - accuracy: 0.9398 - loss: 0.1481 - val_accuracy: 0.7500 - val_loss: 0.3932\n",
            "Epoch 8/20\n",
            "\u001b[1m 60/163\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 717ms/step - accuracy: 0.9447 - loss: 0.1413"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/datasets/pneumonia_resnet_finetuned.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-0aSNX2zJ_Q",
        "outputId": "1c123ca6-c149-420d-b834-34b5b11a49ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_gen)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0hlNlLFzRwM",
        "outputId": "7746a6d4-350d-4563-a5f0-ace1649fc91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 345ms/step - accuracy: 0.8639 - loss: 0.3376\n",
            "Test Accuracy: 0.8573718070983887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YN4B2dE1zRiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}